I0604 15:36:20.046000 1239504 torch/_inductor/config.py:714] compile_threads set to 32
I0604 15:36:21.215000 1239504 torch/_inductor/async_compile.py:210] Creating 'subprocess' pool with 32 workers
I0604 15:36:22.713000 1239504 torch/_dynamo/utils.py:1603] [0/0] ChromiumEventLogger initialized with id 1c1fa750-768c-48bf-81bd-bbc4b3e10f69
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] ORIGINAL BYTECODE toy_example /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py line 3 
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]   3           0 RESUME                   0
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]   4           2 LOAD_FAST                0 (x)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]               4 LOAD_METHOD              0 (sin)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              26 PRECALL                  0
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              30 CALL                     0
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              40 STORE_FAST               1 (y)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]   5          42 LOAD_FAST                1 (y)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              44 LOAD_METHOD              1 (cos)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              66 PRECALL                  0
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              70 CALL                     0
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              80 STORE_FAST               2 (z)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]   6          82 LOAD_FAST                2 (z)
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              84 RETURN_VALUE
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
V0604 15:36:22.715000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
I0604 15:36:22.716000 1239504 torch/_dynamo/symbolic_convert.py:3322] [0/0] Step 1: torchdynamo start tracing toy_example /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:3
I0604 15:36:22.717000 1239504 torch/fx/experimental/symbolic_shapes.py:3334] [0/0] create_env
I0604 15:36:22.730000 1239504 torch/_dynamo/symbolic_convert.py:3679] [0/0] Step 1: torchdynamo done tracing toy_example (RETURN_VALUE)
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code] TRACED GRAPH
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]  ===== pre insert_deferred_runtime_asserts __compiled_fn_1 =====
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]  <eval_with_key>.0 class GraphModule(torch.nn.Module):
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]     def forward(self, L_x_: "f32[1000]"):
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         l_x_ = L_x_
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         y: "f32[1000]" = l_x_.sin();  l_x_ = None
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         z: "f32[1000]" = y.cos();  y = None
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         return (z,)
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code]         
V0604 15:36:22.733000 1239504 torch/fx/passes/runtime_assert.py:118] [0/0] [__graph_code] 
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] TRACED GRAPH
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  ===== __compiled_fn_1 =====
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]  /mnt/home/douliyang/mlsys/Pytorch-Sample/dly/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]     def forward(self, L_x_: "f32[1000][1]cuda:0"):
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         l_x_ = L_x_
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         y: "f32[1000][1]cuda:0" = l_x_.sin();  l_x_ = None
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         z: "f32[1000][1]cuda:0" = y.cos();  y = None
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         return (z,)
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code]         
V0604 15:36:22.735000 1239504 torch/_dynamo/output_graph.py:1408] [0/0] [__graph_code] 
I0604 15:36:22.736000 1239504 torch/_dynamo/output_graph.py:1515] [0/0] Step 2: calling compiler function inductor
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs] TRACED GRAPH
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]  /mnt/home/douliyang/mlsys/Pytorch-Sample/dly/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[1000][1]cuda:0"):
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         y: "f32[1000][1]cuda:0" = l_x_.sin();  l_x_ = None
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         z: "f32[1000][1]cuda:0" = y.cos();  y = None
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         return (z,)
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs]         
V0604 15:36:22.736000 1239504 torch/_inductor/compile_fx.py:1826] [0/0] [__pre_grad_graphs] 
I0604 15:36:23.297000 1239802 torch/_inductor/config.py:714] compile_threads set to 32
I0604 15:36:23.400000 1239504 torch/_functorch/_aot_autograd/autograd_cache.py:731] [0/0] AOTAutograd cache miss for key ax2zkogbbcyqpziedvm2m3txzgnqp56rm2vyirpu533mhjw6iqzq
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph] TRACED GRAPH
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]  ===== Joint graph 0 =====
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]  /mnt/home/douliyang/mlsys/Pytorch-Sample/dly/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class joint_helper(torch.nn.Module):
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]     def forward(self, primals, tangents):
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         primals_1: "f32[1000][1]cuda:0"; tangents_1: "f32[1000][1]cuda:0"; 
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]     
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         primals_1, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         sin: "f32[1000][1]cuda:0" = torch.ops.aten.sin.default(primals_1)
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         cos: "f32[1000][1]cuda:0" = torch.ops.aten.cos.default(sin)
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         sin_1: "f32[1000][1]cuda:0" = torch.ops.aten.sin.default(sin);  sin = None
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         neg: "f32[1000][1]cuda:0" = torch.ops.aten.neg.default(sin_1);  sin_1 = None
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         mul: "f32[1000][1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, neg);  tangents_1 = neg = None
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         cos_1: "f32[1000][1]cuda:0" = torch.ops.aten.cos.default(primals_1);  primals_1 = None
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         mul_1: "f32[1000][1]cuda:0" = torch.ops.aten.mul.Tensor(mul, cos_1);  mul = cos_1 = None
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         return pytree.tree_unflatten([cos, mul_1], self._out_spec)
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph]         
I0604 15:36:23.434000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:791] [0/0] [__aot_joint_graph] 
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code] TRACED GRAPH
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]  ===== tensorify_python_scalars =====
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]  /mnt/home/douliyang/mlsys/Pytorch-Sample/dly/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class joint_helper(torch.nn.Module):
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]     def forward(self, primals, tangents):
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         primals_1: "f32[1000]"; tangents_1: "f32[1000]"; 
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]     
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         primals_1, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         sin: "f32[1000]" = torch.ops.aten.sin.default(primals_1)
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         cos: "f32[1000]" = torch.ops.aten.cos.default(sin)
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         sin_1: "f32[1000]" = torch.ops.aten.sin.default(sin);  sin = None
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         neg: "f32[1000]" = torch.ops.aten.neg.default(sin_1);  sin_1 = None
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         mul: "f32[1000]" = torch.ops.aten.mul.Tensor(tangents_1, neg);  tangents_1 = neg = None
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         cos_1: "f32[1000]" = torch.ops.aten.cos.default(primals_1);  primals_1 = None
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         mul_1: "f32[1000]" = torch.ops.aten.mul.Tensor(mul, cos_1);  mul = cos_1 = None
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         return pytree.tree_unflatten([cos, mul_1], self._out_spec)
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code]         
V0604 15:36:23.436000 1239504 torch/fx/passes/_tensorify_python_scalars.py:364] [0/0] [__graph_code] 
I0604 15:36:23.653000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:895] [0/0] [__aot_graphs] aot_config id: 0, fw_metadata=ViewAndMutationMeta(input_info=[InputAliasInfo(is_leaf=True, mutates_data=False, mutates_metadata=False, mutations_hidden_from_autograd=True, mutations_under_no_grad_or_inference_mode=False, mutation_inductor_storage_resize=False, mutates_storage_metadata=False, requires_grad=True, keep_input_mutations=True)], output_info=[OutputAliasInfo(output_type=<OutputType.non_alias: 1>, raw_type=<class 'torch._subclasses.functional_tensor.FunctionalTensor'>, base_idx=None, dynamic_dims=set(), requires_grad=True, functional_tensor=None)], num_intermediate_bases=0, keep_input_mutations=True, traced_tangents=[FakeTensor(..., device='cuda:0', size=(1000,))], subclass_inp_meta=[PlainTensorMeta(unwrapped_idx=0, memory_format=None)], subclass_fw_graph_out_meta=[PlainTensorMeta(unwrapped_idx=0, memory_format=None)], subclass_tangent_meta=[PlainTensorMeta(unwrapped_idx=0, memory_format=torch.contiguous_format)], is_train=True, traced_tangent_metas=None, num_symints_saved_for_bw=0, grad_enabled_mutation=None, deterministic=False, static_input_indices=[], tokens={}, indices_of_inputs_that_requires_grad_with_mutations_in_bw=[], bw_donated_idxs=[], num_backward_tokens=0, num_graphsafe_rng_states=0, graphsafe_rng_state_index=None), inner_meta=ViewAndMutationMeta(input_info=[InputAliasInfo(is_leaf=True, mutates_data=False, mutates_metadata=False, mutations_hidden_from_autograd=True, mutations_under_no_grad_or_inference_mode=False, mutation_inductor_storage_resize=False, mutates_storage_metadata=False, requires_grad=True, keep_input_mutations=True)], output_info=[OutputAliasInfo(output_type=<OutputType.non_alias: 1>, raw_type=<class 'torch._subclasses.functional_tensor.FunctionalTensor'>, base_idx=None, dynamic_dims=set(), requires_grad=True, functional_tensor=None)], num_intermediate_bases=0, keep_input_mutations=True, traced_tangents=[FakeTensor(..., device='cuda:0', size=(1000,))], subclass_inp_meta=[PlainTensorMeta(unwrapped_idx=0, memory_format=None)], subclass_fw_graph_out_meta=[PlainTensorMeta(unwrapped_idx=0, memory_format=None)], subclass_tangent_meta=[PlainTensorMeta(unwrapped_idx=0, memory_format=torch.contiguous_format)], is_train=True, traced_tangent_metas=None, num_symints_saved_for_bw=0, grad_enabled_mutation=None, deterministic=False, static_input_indices=[], tokens={}, indices_of_inputs_that_requires_grad_with_mutations_in_bw=[], bw_donated_idxs=[], num_backward_tokens=0, num_graphsafe_rng_states=0, graphsafe_rng_state_index=None)
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs] TRACED GRAPH
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]  ===== Forward graph 0 =====
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]  /mnt/home/douliyang/mlsys/Pytorch-Sample/dly/lib/python3.11/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]     def forward(self, primals_1: "f32[1000][1]cuda:0"):
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]         sin: "f32[1000][1]cuda:0" = torch.ops.aten.sin.default(primals_1)
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]         
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]         cos: "f32[1000][1]cuda:0" = torch.ops.aten.cos.default(sin);  sin = None
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]         return (cos, primals_1)
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs]         
I0604 15:36:23.654000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1001] [0/0] [__aot_graphs] 
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs] TRACED GRAPH
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]  ===== Backward graph 0 =====
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]  <eval_with_key>.3 class GraphModule(torch.nn.Module):
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]     def forward(self, primals_1: "f32[1000][1]cuda:0", tangents_1: "f32[1000][1]cuda:0"):
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         sin: "f32[1000][1]cuda:0" = torch.ops.aten.sin.default(primals_1)
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:5 in toy_example, code: z = y.cos()
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         sin_1: "f32[1000][1]cuda:0" = torch.ops.aten.sin.default(sin);  sin = None
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         neg: "f32[1000][1]cuda:0" = torch.ops.aten.neg.default(sin_1);  sin_1 = None
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         mul: "f32[1000][1]cuda:0" = torch.ops.aten.mul.Tensor(tangents_1, neg);  tangents_1 = neg = None
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]          # File: /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:4 in toy_example, code: y = x.sin()
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         cos_1: "f32[1000][1]cuda:0" = torch.ops.aten.cos.default(primals_1);  primals_1 = None
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         mul_1: "f32[1000][1]cuda:0" = torch.ops.aten.mul.Tensor(mul, cos_1);  mul = cos_1 = None
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         return (mul_1,)
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs]         
I0604 15:36:23.655000 1239504 torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:1012] [0/0] [__aot_graphs] 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] FX graph cache hash details for key fypacocn6huyv4bffgkorp5cepcwzxk6rct2f2327fjaalbyz2o2:
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [d56yptsvrs7c22dkp5grqw5j3uzhc2o2y3an4dx2g6ycwfy7wqd] gm: GraphModule()
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] def forward(self, primals_1):
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0]     sin = torch.ops.aten.sin.default(primals_1)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0]     cos = torch.ops.aten.cos.default(sin);  sin = None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0]     return (cos, primals_1)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0]     
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] # To see more debug info, please use `graph_module.print_readable()`
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [23whcb3tt2dosuji7vfazet6ufbcowxr45l7lw6icfuhirp7wvy] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([1000]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_inference]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [qiptf2633zubseuei4bkisoq3not35l6lud6p23p4qmcsxiw2uq] cuda_matmul_settings: (False, True, True)
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [uoy54ulvy55gmjsxg5fufzzoouzlx23ph5oxkxm7uoae6f5bpcu] torch_version: <bytes>
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [66eras4vxnp5s74z5nypcndrswbohyeerdwbfp3zsswag7yiayn] system_info[device]: {'name': 'NVIDIA GeForce RTX 3090'}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [wrqmxdut4gvbentxxvsa5u4mawfp2chh4v2xce4pthuqedouzuo] system_info[version]: {'triton': '3.3.12e234c1d93a7cae949341b60b8567f825914128ed598e1d846c8bc19a9d65ad8-364a7d1dd5f29867a741138f83c5b453259e240a5d51f364f24f7196cedbf442-2e234c1d93a7cae949341b60b8567f825914128ed598e1d846c8bc19a9d65ad8-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-ca6686d24a6f780b8449b43d419d11c978ebd00ab87a5fc6e57198a2027680d0-00deb4ba92653e089ad09c2f67cbc85602c84cd3ee2347ddcfcccc2081cfa45e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-78995dd40c4e54964f62cdc6d47e66f6d9df8b55b172ad7eb99235f27999c840-f7c26e8ffdaf8cd2491de04217d32827b1a4f859f7f93ea56e26590c9f5e071a-e15d644ee2c3fdabf94eb5fb0e09a40cfcfec282bfee4fdfe1604a4402052a54-6e4a7df0c1f6cb799f488ee1d6efd3df754fc18aac6e7209923bb022c36c7c4e-f983f9d6d6f987af520297c8fe3185601ae8f7d60bacab880ac9326bdfee1f67-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-26a8aeaf21e759191f173b578316e243ad970387442a1b238481c6a1b2eecdc4-bd364752852b76a0b75a9d93ecbc239781b730aa75021445a02d795aa8d38f6a-72bc1771d8b160fbafcd5390d1ee6cb72440c10ad4834451465a9e93b42f5d1c-7b506c955ee5646862dae548968e6523d75c37ad4750c214d9ab0f6918ecc88a-89e4844782de5fdff4868ac2846514570a6e280a6b92e91af2e180904043c833-115ada51f797cd098ddc5e4b95e8369a885571b15639694b6801f59e46eab55e-0e48b5e1e95136642ccfe62dc3d0a739a2c20a7b5ee13e9c23c6cecd68cdeb70-b616015f724e553348f5b019f7324dec130f7bbaf984d43300fa69c7c2fdda2f-54fe722cbe379a55695ab9478e73d344377cf5e9d6e055aff7cd03bf6fff1b2a-10285555cd515e21ca54714fc8eb9c173cca6b092b5e951a17ae5eee28ed2707-f2d4e73182e68eddc6237577b2158b7d8498ccb3c50642b9c74c55d3f4be3943', 'cuda': '11.8'}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [dr3cdzo36xqzfqrpt4tb36vezuc23lrvakhzxdti7kpkhtxn7y2] system_info[hash]: e32eae5ce704c49fe2b1f189caab0ae092a796ce32272d5227ce840f3ea0bcf4
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[fx_graph_cache]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[fx_graph_remote_cache]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_local_cache]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[autotune_remote_cache]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_disable_caches]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[custom_op_default_layout_constraint]: needs_fixed_stride_order
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_pre_pass]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[joint_custom_post_pass]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ssupi7bu3rrhdpg2jyegzncu3kg3nnhklyliqvutaxgs7y7k3dx] inductor_config[reorder_for_compute_comm_overlap_passes]: ['reorder_compute_for_overlap', 'sink_waits', 'raise_comms']
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[graph_partition]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[autotune_fallback_to_aten]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [bh33ranllcgilhgmgr3qvygzxjm6isq5iexnfm3zx6fnr2zwlp2] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 1.0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [pwoh5aypf4fxbntdvwt67rppxorqos6xr3w7qzeun6kblbfg2ga] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 2.0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[loop_ordering_after_fusion]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernel_foreach_dynamic_shapes]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_nightly_or_source]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[developer_warnings]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [g7rrnbg5yonzux3cfj5ovre5lob3ayda7qcfpxjvtwmiz4uicii] inductor_config[cpp.min_chunk_size]: 4096
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tuax46wac7rfv2trf5gcps6vleo3cq44lbnrdxtprvo3ljjaddj] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 50
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[triton.max_tiles]: 2
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.compile_wrapper_with_O0]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package_cpp_only]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [xaicuyqjstadzph6cgvxowlzizkts6kzmfupsnbyaorxh37cppz] inductor_config[cuda.cutlass_max_profiling_swizzle_options]: [1, 2, 4]
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.generate_test_runner]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_denylist_regex]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cuda.cutlass_instantiation_level]: 0
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [3w3j4h2iiu3addwyb5alaeecz3so7teb23hp4d5n3b46w5n73ur] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942']
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
V0604 15:36:23.665000 1239504 torch/_inductor/codecache.py:868] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
V0604 15:36:23.666000 1239504 torch/_inductor/triton_bundler.py:237] [0/0] Bailing out TritonBundler.read_and_emit, /tmp/torchinductor_douliyang/triton/0/QOLF45XSH4UHUJ46TCGNELG45VJK6GPGLU3ECKEKP5A3JVMRX7KA is non empty
V0604 15:36:23.666000 1239504 torch/_inductor/triton_bundler.py:237] [0/0] Bailing out TritonBundler.read_and_emit, /tmp/torchinductor_douliyang/triton/0/TJAVHW74KFJECP67DDRDRDRS7WNOJZYFDSQ27BTS22CXENFPDC3A is non empty
V0604 15:36:23.696000 1239504 torch/_inductor/runtime/triton_heuristics.py:216] [0/0] CachingAutotuner gets 1 configs for triton_poi_fused_cos_sin_0
V0604 15:36:23.696000 1239504 torch/_inductor/runtime/triton_heuristics.py:222] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, num_buffers_warp_spec: 0, num_consumer_groups: 0, reg_dec_producer: 0, reg_inc_consumer: 0, maxnreg: None
V0604 15:36:23.697000 1239504 torch/_inductor/runtime/triton_heuristics.py:231] [0/0] Triton cache dir: /tmp/torchinductor_douliyang/triton/0
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] Output code: 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # AOT ID: ['0_forward']
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import torch
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import math
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import random
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import os
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import tempfile
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from math import inf, nan
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from cmath import nanj
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch import device, empty_strided
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.codegen.multi_kernel import MultiKernelCall
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton.language as tl
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] aten = torch.ops.aten
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] _quantized = torch.ops._quantized
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] async_compile = AsyncCompile()
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # kernel path: /tmp/torchinductor_douliyang/vo/cvomcf7f7rcre7rbhb623rqnw65prxqqgj42vnolwsf3wac3pvj7.py
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # Topologically Sorted Source Nodes: [y, z], Original ATen: [aten.sin, aten.cos]
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # Source node to ATen node mapping:
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   y => sin
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   z => cos
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] # Graph fragment:
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%primals_1,), kwargs = {})
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] #   %cos : [num_users=1] = call_function[target=torch.ops.aten.cos.default](args = (%sin,), kwargs = {})
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] triton_poi_fused_cos_sin_0 = async_compile.triton('triton_poi_fused_cos_sin_0', '''
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] import triton.language as tl
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] @triton_heuristics.pointwise(
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     size_hints={'x': 1024}, 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     filename=__file__,
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=82, cc=86, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=1536, warp_size=32), 'constants': {}, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]]}]},
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cos_sin_0', 'mutated_arg_names': [], 'optimize_mem': False, 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '3467EE107B69AF55EB3A4F3E81CC444B65CAFE357D3A7F583605C43E546F325C', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     min_elem_per_thread=0
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] )
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] @triton.jit
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] def triton_poi_fused_cos_sin_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xnumel = 1000
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     xmask = xindex < xnumel
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     x0 = xindex
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), xmask)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp1 = tl_math.sin(tmp0)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tmp2 = tl_math.cos(tmp1)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp2, xmask)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] ''', device_str='cuda')
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] async_compile.wait(globals())
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] del async_compile
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] def call(args):
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     primals_1, = args
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     args.clear()
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     assert_size_stride(primals_1, (1000, ), (1, ))
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         torch.cuda.set_device(0)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         buf0 = empty_strided_cuda((1000, ), (1, ), torch.float32)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         # Topologically Sorted Source Nodes: [y, z], Original ATen: [aten.sin, aten.cos]
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]         triton_poi_fused_cos_sin_0.run(primals_1, buf0, 1000, stream=stream0)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     return (buf0, primals_1, )
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     primals_1 = rand_strided((1000, ), (1, ), device='cuda:0', dtype=torch.float32)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     fn = lambda: call([primals_1])
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] if __name__ == "__main__":
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V0604 15:36:23.709000 1239504 torch/_inductor/codecache.py:1093] [0/0] [__output_code] 
V0604 15:36:23.710000 1239504 torch/_inductor/codecache.py:1094] [0/0] [__output_code] Output code written to: /tmp/torchinductor_douliyang/r3/cr3yloqakst4rnmtad2z7hxl2txdkcv3tgzum4bvzigqg67k55rv.py
I0604 15:36:23.710000 1239504 torch/_inductor/codecache.py:1306] [0/0] fx graph cache hit for key fypacocn6huyv4bffgkorp5cepcwzxk6rct2f2327fjaalbyz2o2
V0604 15:36:23.711000 1239504 torch/_inductor/compile_fx.py:834] [0/0] FX codegen and compilation took 0.054s
I0604 15:36:23.711000 1239504 torch/_inductor/compile_fx.py:837] [0/0] Overview info of inductor aten mms: 
I0604 15:36:23.711000 1239504 torch/_inductor/compile_fx.py:848] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
I0604 15:36:23.716000 1239504 torch/_dynamo/output_graph.py:1520] [0/0] Step 2: done compiler function inductor
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] MODIFIED BYTECODE toy_example /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py line 3 
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]   3           0 RESUME                   0
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]               2 LOAD_GLOBAL              5 (NULL + __compiled_fn_1)
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              14 LOAD_FAST                0 (x)
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              16 COPY                     1
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              18 STORE_FAST               3 (tmp_0)
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              20 PRECALL                  1
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              24 CALL                     1
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              34 UNPACK_SEQUENCE          1
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode]              38 RETURN_VALUE
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
V0604 15:36:23.717000 1239504 torch/_dynamo/convert_frame.py:781] [0/0] [__bytecode] 
I0604 15:36:23.718000 1239504 torch/fx/experimental/symbolic_shapes.py:4734] [0/0] produce_guards
I0604 15:36:23.719000 1239504 torch/_dynamo/pgo.py:660] [0/0] put_code_state: no cache key, skipping
I0604 15:36:23.720000 1239504 torch/_dynamo/convert_frame.py:1121] [0/0] run_gc_after_compile: running gc
I0604 15:36:25.335000 1239802 torch/_inductor/remote_cache.py:417] Cache Metrics: None
I0604 15:36:25.335000 1239802 torch/_inductor/remote_cache.py:417] 
I0604 15:36:25.336000 1239802 torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [
I0604 15:36:25.336000 1239802 torch/_dynamo/eval_frame.py:475] 
I0604 15:36:25.336000 1239802 torch/_dynamo/eval_frame.py:475] ]
I0604 15:36:25.337000 1239802 torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:
I0604 15:36:25.337000 1239802 torch/_dynamo/utils.py:765] Function, Runtimes (s)
I0604 15:36:25.944000 1239504 torch/_inductor/remote_cache.py:417] Cache Metrics:
I0604 15:36:25.944000 1239504 torch/_inductor/remote_cache.py:417]   LocalAutotuneCache: {hit: 1, miss: 0, put: 0, exception: 0}
I0604 15:36:25.944000 1239504 torch/_inductor/remote_cache.py:417]   backend:_LocalAutotuneCacheBackend: {hit: 1, miss: 0, put: 0, exception: 0}
I0604 15:36:25.944000 1239504 torch/_inductor/remote_cache.py:417] 
I0604 15:36:25.945000 1239504 torch/_dynamo/eval_frame.py:475] TorchDynamo attempted to trace the following frames: [
I0604 15:36:25.945000 1239504 torch/_dynamo/eval_frame.py:475]   * toy_example /mnt/home/douliyang/mlsys/Pytorch-Sample/Pytorch2.0/pytorch-triton.py:3
I0604 15:36:25.945000 1239504 torch/_dynamo/eval_frame.py:475] ]
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] TorchDynamo compilation metrics:
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] Function, Runtimes (s)
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] _compile.compile_inner, 1.0046
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] OutputGraph.call_user_compiler, 0.9801
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] _recursive_pre_grad_passes, 0.0043
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] inductor_codecache_torch_key, 0.1758
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] create_aot_dispatcher_function, 0.3146
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] _recursive_joint_graph_passes, 0.1948
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] compile_fx.<locals>.fw_compiler_base, 0.0558
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] compile_fx_inner, 0.0551
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] TritonBundler.read_and_emit, 0.0007
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] PyCodeCache.load_by_key_path, 0.0424
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] async_compile.precompile, 0.0349
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] async_compile.wait, 0.0002
I0604 15:36:25.945000 1239504 torch/_dynamo/utils.py:765] gc, 0.0011
